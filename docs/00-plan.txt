## PROJECT: ETL pipeline azure storage to mongodb
    - This project involves creating an ETL pipeline that extracts data from Azure Storage, transforms it as needed, and loads it into a MongoDB database.

## NOTE 
    - Azure Functions will be used to implement the ETL pipeline.
    - The pipeline will be triggered by events in Azure Storage (e.g., new blobs being added).
    - Data transformation will be performed using Python and relevant libraries (e.g., Pandas).
    - The final output will be stored in a MongoDB database hosted on Azure.

## TASK LIST 
    - [x] Set up Azure Storage account and container.
    - [x] Create a MongoDB database on Atlas
    - [x] Develop Azure Function to trigger on blob creation or timetigger
    - [x] Implement data extraction logic in the Azure Function.
    - [x] mock data
            | **Field Name** | **Description** |
            | --- | --- |
            | CRM_ID | Customer rm_id |
            | CL4 | Masked 16-digit card number |
            | CARD_MAILING_TRACKING_NO | Card mailing tracking number |
            | CARD_MAILING_DATE | Card mailing date (วันที่ส่งไปรษณีย์) |
    - [x] Load transformed data into MongoDB.
    - [x] Test the entire ETL pipeline end-to-end.
    - [x] Document the project setup, configuration, and usage instructions.
    - [x] Deploy the Azure Function to Azure.
        - [x] create script deploy_function.sh
        - [x] Run the script to deploy the Azure Function.
    - [x] test upload fine
    - [x] Set up monitoring and logging for the Azure Function.
    - [x] Implement error handling and retry logic in the Azure Function.
        - ใช้ Azure Function Retry Policy
    - [x] เพิ่ม trigger ให้รันตามเวลาที่กำหนด (เช่น ทุกวันเวลา 00:00 น.)
    - [x] เพิ่ม trigger by http request
    - [x] เพิ่มการจัดการ environment variables ใน Azure Function
    - [ ] Optimize the ETL pipeline for performance and scalability.
        - Implement batch insert for large DataFrames (e.g., insert 500-1000 records per batch to reduce memory spike and timeout)
        - Add parallel processing for multiple large files using Azure Durable Functions or fan-out/fan-in pattern
        - Optimize Pandas usage by reading large files in chunks (e.g., using chunksize in pd.read_csv)
        - Use global MongoClient or connection pooling to reduce connection overhead
        - Configure resource limits, timeout, and retry policy in Azure Function for optimal workload handling
        - Enhance error handling with retry logic for recoverable errors (e.g., network errors)
        - Add detailed logging for performance monitoring (e.g., log time spent in each step)

## CURRENT GOALS
